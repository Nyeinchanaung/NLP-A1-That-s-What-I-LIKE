{"cells":[{"cell_type":"markdown","metadata":{"id":"hDNe4buKwGy_"},"source":["# GloVE\n","\n","Let's work on implementation of GloVE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H27xWfPuwGzB"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALMH7eyOykFe"},"outputs":[],"source":["# # change the runtime type to GPU Cuda\n","# import torch\n","\n","# if torch.cuda.is_available():\n","#     device = torch.device(\"cuda\")\n","#     print(\"GPU is available\")\n","# else:\n","#     device = torch.device(\"cpu\")\n","#     print(\"GPU is not available\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23453,"status":"ok","timestamp":1737348816148,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"},"user_tz":-420},"id":"AO_0M_1kZGjr","outputId":"2ba6a75b-583d-442f-f077-d36b4612fda2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kb9vU3H3ZLTV"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/_NLP/NLP-A1-That-s-What-I-LIKE-st125553')"]},{"cell_type":"markdown","metadata":{"id":"6umM2ONkwGzC"},"source":["## 1. Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6709,"status":"ok","timestamp":1737348822853,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"},"user_tz":-420},"id":"0GrT_t0SwGzC","outputId":"3c3ac735-a73a-40aa-a0e3-62e014669e5e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Load nltk\n","import nltk\n","\n","# download news category dataset from nltk\n","nltk.download('brown') # download brown corpus\n","nltk.download('punkt') # download punkt for tokenization\n","nltk.download('punkt_tab') # download punkt_tab for tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_JWlZ2QwGzD"},"outputs":[],"source":["#1. tokenization\n","# import the news category dataset\n","from nltk.corpus import brown\n","corpus = brown.sents(categories='news')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMRuXP2twGzD"},"outputs":[],"source":["#get word sequences and unique words\n","flatten = lambda l: [item for sublist in l for item in sublist]\n","vocab = list(set(flatten(corpus)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737348823428,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"},"user_tz":-420},"id":"dNbExuVvwGzD","outputId":"46a751d0-c82e-4f1c-f405-8b9e6f919b2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["before vocabs_len: 14394\n","after vocabs_len: 14395\n"]}],"source":["#2. numeralization\n","#find unique words\n","flatten = lambda l: [item for sublist in l for item in sublist]\n","#assign unique integer\n","vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>\n","\n","print(f\"before vocabs_len: {len(vocabs)}\")\n","vocabs.append('<UNK>')\n","print(f\"after vocabs_len: {len(vocabs)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1737348823428,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"},"user_tz":-420},"id":"VNQAMib6wGzD","outputId":"55375fc5-a727-4e02-815a-60ff0139555c"},"outputs":[{"data":{"text/plain":["14394"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#create handy mapping between integer and word\n","word2index = {v:idx for idx, v in enumerate(vocabs)}\n","word2index['<UNK>']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1737348823429,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"},"user_tz":-420},"id":"0P-QTLLSwGzE","outputId":"043d3cf9-0374-471c-da05-190f66227f00"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<UNK>'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\n","index2word = {v:k for k, v in word2index.items()}\n","index2word[14394]"]},{"cell_type":"markdown","metadata":{"id":"pQnE9c85wGzE"},"source":["## 2. Build Co-occurence Matrix X"]},{"cell_type":"markdown","metadata":{"id":"pP-ex9KhwGzE"},"source":["Here, we need to count the co-occurence of two words given some window size.  We gonna use window size of 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrYmFdNPwGzE"},"outputs":[],"source":["from collections import Counter\n","\n","X_i = Counter(flatten(corpus))\n","#X_i"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wV_vJ-OlwGzE"},"outputs":[],"source":["# window_size = 2\n","# skip_grams = []\n","\n","# for doc in corpus:\n","#     for i in range(window_size, len(doc)-window_size):\n","#         center = doc[i]\n","#         outside = [word2index[doc[i+j]] for j in range(-window_size, window_size+1) if i+j != i]\n","#         for each_out in outside:\n","#             skip_grams.append((center, each_out))\n","# #skip_grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-gMJLgCxXRD"},"outputs":[],"source":["window_size = 2\n","\n","skip_grams = []\n","\n","#loop each corpus\n","for doc in corpus:\n","    #look from the 2nd word until second last word\n","    for i in range(window_size, len(doc)-window_size):\n","        #center word\n","        center = doc[i]\n","        #outside words = 2 words\n","\n","        outside = []\n","        for j in range(window_size):\n","            outside.append(doc[i+(j+1)])\n","            outside.append(doc[i-(j+1)])\n","\n","        #for each of these two outside words, we gonna append to a list\n","        for each_out in outside:\n","            skip_grams.append((center, each_out))\n","            #center, outside1;   center, outside2\n","\n","#skip_grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZ-3O5rSwkDX"},"outputs":[],"source":["#skip_grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZzlsfKIwGzE"},"outputs":[],"source":["X_ik_skipgrams = Counter(skip_grams)\n","#X_ik_skipgrams"]},{"cell_type":"markdown","metadata":{"id":"82hFDH6ZwGzE"},"source":["### Weighting function\n","\n","GloVe includes a weighting function to scale down too frequent words.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"im3DvdvSwGzE"},"outputs":[],"source":["def weighting(w_i, w_j, X_ik):\n","\n","    #check whether the co-occurences between w_i and w_j is available\n","    try:\n","        x_ij = X_ik[(w_i, w_j)]\n","        #if not exist, then set to 1 \"laplace smoothing\"\n","    except:\n","        x_ij = 1\n","\n","    #set xmax\n","    x_max = 100\n","    #set alpha\n","    alpha = 0.75\n","\n","    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n","    if x_ij < x_max:\n","        result = (x_ij / x_max)**alpha\n","    #otherwise, set to 1\n","    else:\n","        result = 1\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aPJ04vtEwGzE"},"outputs":[],"source":["from itertools import combinations_with_replacement\n","\n","X_ik = {} #keeping the co-occurences\n","weighting_dic = {} #already scale the co-occurences using the weighting function\n","\n","for bigram in combinations_with_replacement(vocab, 2):\n","    if X_ik_skipgrams.get(bigram):  #if the pair exists in our corpus\n","        co = X_ik_skipgrams[bigram]\n","        X_ik[bigram] = co + 1 #for stability\n","        X_ik[(bigram[1], bigram[0])] = co + 1 #basically apple, banana = banana, apple\n","    else:\n","        pass\n","\n","    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n","    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"]},{"cell_type":"markdown","metadata":{"id":"eEbPamAVwGzE"},"source":["## 3. Prepare train data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mbY2MHcZwGzE"},"outputs":[],"source":["import math\n","\n","def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n","\n","    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n","\n","    #convert our skipgrams to id\n","    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n","\n","    #randomly choose indexes based on batch size\n","    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n","\n","    #get the random input and labels\n","    for index in random_index:\n","        random_inputs.append([skip_grams_id[index][0]])\n","        random_labels.append([skip_grams_id[index][1]])\n","        #coocs\n","        pair = skip_grams[index] #e.g., ('banana', 'fruit')\n","        try:\n","            cooc = X_ik[pair]\n","        except:\n","            cooc = 1\n","        random_coocs.append([math.log(cooc)])\n","\n","        #weightings\n","        weighting = weighting_dic[pair]\n","        random_weightings.append([weighting])\n","\n","    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"]},{"cell_type":"markdown","metadata":{"id":"FWW8Y3KWwGzF"},"source":["### Testing the method"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HaOr1x4gvkBr"},"outputs":[],"source":["#weighting_dic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FKSDx8FLv0QQ","outputId":"ab921568-58fa-4164-e7b7-3d1499c0c484"},"outputs":[{"data":{"text/plain":["('County', 'Grand')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["skip_grams[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"967f7YHWwPqY"},"outputs":[],"source":["#skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QLsbYfvRwGzF"},"outputs":[],"source":["batch_size = 2\n","x, y, cooc, weighting = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Lxg9-09owGzF","outputId":"68fb8208-682b-48ee-f032-5f1b6e83f139"},"outputs":[{"data":{"text/plain":["array([[10204],\n","       [12718]])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4hSJTeFzwGzF","outputId":"104c6888-a03f-4d1d-8b08-ee8f182f4cbf"},"outputs":[{"data":{"text/plain":["array([[14132],\n","       [11992]])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vhiU_tFYwGzF","outputId":"f4d82ae7-030b-4d12-b8af-9b2277b509fd"},"outputs":[{"data":{"text/plain":["array([[1.09861229],\n","       [1.09861229]])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["cooc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L7wwbV0kwGzF","outputId":"e72d2297-049a-43f3-8673-e33db55a86a2"},"outputs":[{"data":{"text/plain":["array([[0.07208434],\n","       [0.07208434]])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["weighting"]},{"cell_type":"markdown","metadata":{"id":"TGSk1RwrwGzF"},"source":["## 4. Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9o5IQM0JwGzF"},"outputs":[],"source":["class Glove(nn.Module):\n","\n","    def __init__(self, voc_size, emb_size, word2index):\n","        super(Glove, self).__init__()\n","        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n","        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n","\n","        self.center_bias       = nn.Embedding(voc_size, 1)\n","        self.outside_bias      = nn.Embedding(voc_size, 1)\n","\n","        self.word2index = word2index\n","\n","    def forward(self, center, outside, coocs, weighting):\n","        center_embeds  = self.center_embedding(center) #(batch_size, 1, emb_size)\n","        outside_embeds = self.outside_embedding(outside) #(batch_size, 1, emb_size)\n","\n","        center_bias    = self.center_bias(center).squeeze(1)\n","        target_bias    = self.outside_bias(outside).squeeze(1)\n","\n","        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n","        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n","\n","        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n","\n","        return torch.sum(loss)\n","\n","    def get_embed(self, word):\n","        try:\n","            index = word2index[word]\n","        except:\n","            index = word2index['<UNK>']\n","\n","        word = torch.LongTensor([index])\n","\n","        embed_c = self.center_embedding(word)\n","        embed_o = self.outside_embedding(word)\n","        embed   = (embed_c + embed_o) / 2\n","\n","        return embed[0][0].item(), embed[0][1].item()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c3AoHCe9peFO","outputId":"ea3531ad-0c30-4d4f-a4ab-1918d32d64a8"},"outputs":[{"data":{"text/plain":["14395"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["len(vocabs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZQOhdhZrwGzF"},"outputs":[],"source":["#test our system\n","\n","voc_size = len(vocabs)\n","emb_size = 2\n","model = Glove(voc_size, emb_size,word2index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NcTY7NQVwGzF"},"outputs":[],"source":["x_tensor = torch.LongTensor(x)\n","y_tensor = torch.LongTensor(y)\n","cooc_tensor = torch.FloatTensor(cooc)\n","weighting_tensor = torch.FloatTensor(weighting)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LRzviSfXwGzF"},"outputs":[],"source":["loss = model(x_tensor, y_tensor, cooc_tensor, weighting_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ndw0hlcgwGzF","outputId":"1e7f4fa8-a4de-4c7e-e257-d84d53f07ce0"},"outputs":[{"data":{"text/plain":["tensor(1.1692, grad_fn=<SumBackward0>)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["loss"]},{"cell_type":"markdown","metadata":{"id":"b-eP3oFNwGzF"},"source":["## 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nl6jmBJtwGzF"},"outputs":[],"source":["batch_size     = 2 # mini-batch size\n","embedding_size = 2 #so we can later plot\n","model          = Glove(voc_size, embedding_size, word2index)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iGCoKKWQwGzF"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"F_Nl6eJfwGzF","outputId":"1dd6d50d-59a5-4609-9f80-786de4652bea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 100 | Loss: 0.208598 | time: 0m 0s\n","Epoch: 200 | Loss: 0.256442 | time: 0m 0s\n","Epoch: 300 | Loss: 0.999286 | time: 0m 0s\n","Epoch: 400 | Loss: 21.913521 | time: 0m 0s\n","Epoch: 500 | Loss: 0.057508 | time: 0m 0s\n","Epoch: 600 | Loss: 0.088874 | time: 0m 0s\n","Epoch: 700 | Loss: 0.659058 | time: 0m 0s\n","Epoch: 800 | Loss: 1.040385 | time: 0m 0s\n","Epoch: 900 | Loss: 0.245381 | time: 0m 0s\n","Epoch: 1000 | Loss: 39.603188 | time: 0m 0s\n","Total time:  1 minutes 21 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","\n","    start = time.time()\n","\n","    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n","    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n","    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n","    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n","    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n","\n","    optimizer.zero_grad()\n","    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    end = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start, end)\n","\n","    if (epoch + 1) % 100 == 0:\n","\n","      end = time.time()\n","      epoch_mins, epoch_secs = epoch_time(start, end)\n","\n","      print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n","      start = end\n","\n","end_time = time.time()\n","minutes, seconds = epoch_time(start_time, end_time)\n","print(f\"Total time: {minutes:2.0f} minutes {seconds:2.0f} seconds\")\n"]},{"cell_type":"markdown","metadata":{"id":"Sp4mavlGwGzF"},"source":["## 6. Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JWT0PaxmOaly"},"outputs":[],"source":["vect = []\n","\n","for word in vocabs:\n","    vect.append(model.get_embed(word))\n","vect = np.array(vect)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wFfW8uVJWzPj"},"outputs":[],"source":["# # ipython-input-54-07b52817ebdf\n","# vect = []\n","\n","# for word in vocabs:\n","#     try:\n","#         vect.append(model.get_embed(word)) # Get embedding for the current word\n","#     except IndexError:\n","#         # Handle IndexError, likely due to the word index being out of range\n","#         # This could happen if the word is not in the model's vocabulary\n","#         print(f\"Word '{word}' caused an IndexError. Skipping...\")\n","#         # Alternatively, you could assign a default embedding for unknown words\n","#         # For example: vect.append(model.get_embed('<UNK>'))\n","# vect = np.array(vect) # Convert the list of embeddings to a numpy array"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BIL3pv6QOlHN"},"outputs":[],"source":["#scipy version\n","from scipy import spatial\n","\n","def cos_sim(a, b):\n","    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n","    return cos_sim\n","\n","def cos_sim_scores(vect_space, target_vect):\n","    scores = []\n","    for each_vect in vect_space:\n","        each_vect = tuple(each_vect)\n","        target_vect=tuple(target_vect)\n","        scores.append(cos_sim(target_vect, each_vect))\n","\n","    return np.array(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FbCCjMPgOpFW"},"outputs":[],"source":["def similarity(model, data):\n","    words = data.split(\" \")\n","\n","    embed0 = np.array(model.get_embed(words[0]))\n","    embed1 = np.array(model.get_embed(words[1]))\n","    embed2 = np.array(model.get_embed(words[2]))\n","\n","    sim_vect = embed1 - embed0 + embed2\n","\n","    sim_scores = cos_sim_scores(vect, sim_vect)\n","    max_score_idx = np.argmax(sim_scores)\n","    sim_word = index2word[max_score_idx]\n","\n","    result = False\n","    if sim_word == words[3]:\n","        result = True\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"AprF82fjOrVa"},"source":["### Semantic Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lX33QMJsOr7q"},"outputs":[],"source":["semantic_file = \"data/word-test-semantic.txt\"\n","# open file\n","with open(semantic_file, \"r\") as file:\n","    sem_file = file.readlines()\n","    #send semantic into vector\n","\n","semantic = []\n","for sent in sem_file:\n","    semantic.append(sent.strip())\n","\n","#semantic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rwySFNmoOt7R"},"outputs":[],"source":["sem_count = len(semantic)\n","#sem_total\n","sem_correct = 0\n","for sent in semantic:\n","    if similarity(model, sent):\n","        sem_correct += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v1hkyzbsOwY0","outputId":"ef001897-b19c-461a-f67b-38c05f62b7d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Semantic accuracy: 0.00\n","Semantic correct: 0\n","Semantic count: 506\n"]}],"source":["sem_accuracy = sem_correct / sem_count\n","print(f\"Semantic accuracy: {sem_accuracy:2.2f}\")\n","print(f\"Semantic correct: {sem_correct}\")\n","print(f\"Semantic count: {sem_count}\")"]},{"cell_type":"markdown","metadata":{"id":"OLa6muRqOzvr"},"source":["### Syntatic Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T2_jSkKPOx7z"},"outputs":[],"source":["syntatic_file = \"data/word-test-syntatic.txt\"\n","# open file\n","with open(syntatic_file, \"r\") as file:\n","    syn_file = file.readlines()\n","\n","syntatic = []\n","for sent in syn_file:\n","    syntatic.append(sent.strip())\n","#syntatic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xsUoKgCwO0df"},"outputs":[],"source":["syn_count = len(syntatic)\n","syn_correct = 0\n","for sent in syntatic:\n","    if similarity(model, sent):\n","        syn_correct += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l8fiQMrwO2L7","outputId":"058d5977-0cbb-44c3-8913-250dba242655"},"outputs":[{"name":"stdout","output_type":"stream","text":["Syntatic accuracy: 0.00\n","Syntatic correct: 0\n","Syntatic count: 1560\n"]}],"source":["syn_accuracy = syn_correct / syn_count\n","print(f\"Syntatic accuracy: {syn_accuracy:2.2f}\")\n","print(f\"Syntatic correct: {syn_correct}\")\n","print(f\"Syntatic count: {syn_count}\")"]},{"cell_type":"markdown","metadata":{"id":"RhhqHCBzO4wx"},"source":["### Similarity Test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HGzep5rMO5fp"},"outputs":[],"source":["similarity_file = \"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n","# open file\n","with open(similarity_file, \"r\") as file:\n","    sim_file = file.readlines()\n","\n","similarity = []\n","for sent in sim_file:\n","    similarity.append(sent.strip())\n","#syntatic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nya3DFZsO7Kb"},"outputs":[],"source":["def similarity_test(model, test_data):\n","    words = test_data.split(\"\\t\")\n","\n","    embed0 = np.array(model.get_embed(words[0].strip()))\n","    embed1 = np.array(model.get_embed(words[1].strip()))\n","\n","    model_result = embed1 @ embed0.T\n","    sim_result = float(words[2].strip())\n","\n","    return sim_result, model_result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xj5kfpCxO8tn"},"outputs":[],"source":["sim_scores = []\n","model_scores = []\n","for sent in similarity:\n","    sim_result, model_result = similarity_test(model, sent)\n","\n","    sim_scores.append(sim_result)\n","    model_scores.append(model_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BqsGDasFO_3u","outputId":"f8157066-ae8a-4954-a5ae-c6daace29fca"},"outputs":[{"name":"stdout","output_type":"stream","text":["The correlation result is 0.06.\n"]}],"source":["from scipy.stats import spearmanr\n","\n","corr = spearmanr(sim_scores, model_scores)[0]\n","\n","print(f\"The correlation result is {corr:2.2f}.\")"]},{"cell_type":"markdown","metadata":{"id":"83OS3kywFono"},"source":["### Test P Value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LPjGODjIA4P8","outputId":"bdaba692-d211-46ed-bcae-6d3b0c769f47"},"outputs":[{"name":"stdout","output_type":"stream","text":["        word1  word2  similarity\n","0       tiger    cat        7.35\n","1       tiger  tiger       10.00\n","2       plane    car        5.77\n","3       train    car        6.31\n","4  television  radio        6.77\n"]}],"source":["import pandas as pd\n","from scipy.stats import spearmanr\n","\n","# Load the dataset\n","file_path = \"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n","similarity_data = pd.read_csv(file_path, sep='\\t', names=['word1', 'word2', 'similarity'])\n","\n","# Display sample data\n","print(similarity_data.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v0LmAuHUA8xg"},"outputs":[],"source":["def compute_dot_product(model, word1, word2):\n","    \"\"\"Compute dot product between embeddings of two words.\"\"\"\n","    embedding1 = model.get_embed(word1)\n","    embedding2 = model.get_embed(word2)\n","    if isinstance(embedding1, tuple) or isinstance(embedding2, tuple):\n","        embedding1 = list(embedding1)\n","        embedding2 = list(embedding2)\n","    return np.dot(embedding1, embedding2)\n","\n","# Compute model-based similarities\n","def compute_model_similarities(model, data):\n","    model_similarities = []\n","    for _, row in data.iterrows():\n","        try:\n","            dot_product = compute_dot_product(model, row['word1'], row['word2'])\n","            model_similarities.append(dot_product)\n","        except KeyError:\n","            # Handle missing words in the model\n","            model_similarities.append(0.0)\n","    return model_similarities\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zmkQNDAgA_RZ","outputId":"c1635c84-d103-41dc-a707-0c59d21f2638"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spearman correlation: 0.0583, p-value: 0.4084\n"]}],"source":["# Prepare data for word embedding models\n","model_similarities = compute_model_similarities(model, similarity_data)\n","gold_standard_similarities = similarity_data['similarity'].values\n","\n","# Calculate Spearman's rank correlation\n","correlation, p_value = spearmanr(model_similarities, gold_standard_similarities)\n","\n","print(f\"Spearman correlation: {correlation:.4f}, p-value: {p_value:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"hQ2RKeBIPkFS"},"source":["\n","## 7. Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3DjncQ1YPtmv"},"outputs":[],"source":["# Saving the model for testing\n","torch.save(model.state_dict(), 'app/models/glove.model')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-kskGsn8P8f3"},"outputs":[],"source":["glove_args = {\n","    'voc_size': voc_size,\n","    'emb_size': emb_size,\n","    'word2index': word2index,\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FSS6s31GP97y"},"outputs":[],"source":["import pickle\n","pickle.dump(glove_args, open('app/models/glove.args', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jSx1RgFhQC9r","outputId":"31322486-09b8-4938-c063-c794ebeab01d"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-56-8fdb8e7de74c>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_glove.load_state_dict(torch.load('app/models/glove.model'))\n"]},{"data":{"text/plain":["(-0.6850587129592896, 0.10881073772907257)"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["glove_args = pickle.load(open('app/models/glove.args', 'rb'))\n","model_glove = Glove(**glove_args)\n","model_glove.load_state_dict(torch.load('app/models/glove.model'))\n","\n","# Test the model\n","model_glove.get_embed('The')"]},{"cell_type":"markdown","metadata":{"id":"IG9XZYin7s4b"},"source":["## 8. Calculate MSE"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEv-UkLZcGie","executionInfo":{"status":"ok","timestamp":1737349750275,"user_tz":-420,"elapsed":465,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"50350757-4598-406b-8943-5b74c554949d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Spearman correlation: 0.0583, p-value: 0.4084\n","Mean Squared Error (MSE): 30.8755, Yture MSE: 0.0000\n","Correlation for Y-True: 1.0000, p-value: 0.0000\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","import pandas as pd\n","from scipy.stats import spearmanr\n","import numpy as np\n","\n","# Load the dataset\n","file_path = \"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n","similarity_data = pd.read_csv(file_path, sep='\\t', names=['word1', 'word2', 'similarity'])\n","\n","def compute_dot_product(model, word1, word2):\n","    \"\"\"Compute dot product between embeddings of two words.\"\"\"\n","    embedding1 = model.get_embed(word1)\n","    embedding2 = model.get_embed(word2)\n","    if embedding1 is None or embedding2 is None:\n","        return 0.0  # Default to zero for missing embeddings\n","    return np.dot(np.array(embedding1), np.array(embedding2))\n","\n","def compute_model_similarities(model, data):\n","    \"\"\"Compute similarities using the model.\"\"\"\n","    model_similarities = []\n","    for _, row in data.iterrows():\n","        dot_product = compute_dot_product(model, row['word1'], row['word2'])\n","        model_similarities.append(dot_product)\n","    return model_similarities\n","\n","# Prepare data for word embedding models\n","gold_standard_similarities = similarity_data['similarity'].values  # Y-true\n","model_similarities = compute_model_similarities(model, similarity_data)\n","\n","# Calculate Spearman's rank correlation\n","correlation, p_value = spearmanr(model_similarities, gold_standard_similarities)\n","\n","# Calculate Mean Squared Error (MSE)\n","mse = mean_squared_error(gold_standard_similarities, model_similarities)\n","ytrue_mse = mean_squared_error(gold_standard_similarities, gold_standard_similarities)\n","\n","# Output results\n","print(f\"Spearman correlation: {correlation:.4f}, p-value: {p_value:.4f}\")\n","print(f\"Mean Squared Error (MSE): {mse:.4f}, Yture MSE: {ytrue_mse:.4f}\")\n","\n","# Calculate correlation for Y-True\n","ytrue_correlation, ytrue_p_value = spearmanr(gold_standard_similarities, gold_standard_similarities)\n","print(f\"Correlation for Y-True: {ytrue_correlation:.4f}, p-value: {ytrue_p_value:.4f}\")\n"]}],"metadata":{"anaconda-cloud":{},"colab":{"machine_shape":"hm","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}