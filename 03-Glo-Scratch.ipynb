{"cells":[{"cell_type":"markdown","metadata":{"id":"hDNe4buKwGy_"},"source":["# GloVE\n","\n","Let's work on implementation of GloVE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H27xWfPuwGzB"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# # change the runtime type to GPU Cuda\n","# import torch\n","\n","# if torch.cuda.is_available():\n","#     device = torch.device(\"cuda\")\n","#     print(\"GPU is available\")\n","# else:\n","#     device = torch.device(\"cpu\")\n","#     print(\"GPU is not available\")\n","\n"],"metadata":{"id":"ALMH7eyOykFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AO_0M_1kZGjr","executionInfo":{"status":"ok","timestamp":1737269100999,"user_tz":-420,"elapsed":2535,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"65b8830f-abea-4471-a7a4-4860335959c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/_NLP/NLP-A1-That-s-What-I-LIKE-st125553')"],"metadata":{"id":"kb9vU3H3ZLTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6umM2ONkwGzC"},"source":["## 1. Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GrT_t0SwGzC","executionInfo":{"status":"ok","timestamp":1737269103763,"user_tz":-420,"elapsed":2767,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"87d1cfe7-dbd8-44fc-d798-8b53311f69f7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["# Load nltk\n","import nltk\n","\n","# download news category dataset from nltk\n","nltk.download('brown') # download brown corpus\n","nltk.download('punkt') # download punkt for tokenization\n","nltk.download('punkt_tab') # download punkt_tab for tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_JWlZ2QwGzD"},"outputs":[],"source":["#1. tokenization\n","# import the news category dataset\n","from nltk.corpus import brown\n","corpus = brown.sents(categories='news')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMRuXP2twGzD"},"outputs":[],"source":["#get word sequences and unique words\n","flatten = lambda l: [item for sublist in l for item in sublist]\n","vocab = list(set(flatten(corpus)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNbExuVvwGzD","executionInfo":{"status":"ok","timestamp":1737274540771,"user_tz":-420,"elapsed":2,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"b5d6a2f0-db8e-4708-8bb3-395fa4dc92db"},"outputs":[{"output_type":"stream","name":"stdout","text":["before vocabs_len: 14394\n","after vocabs_len: 14395\n"]}],"source":["#2. numeralization\n","#find unique words\n","flatten = lambda l: [item for sublist in l for item in sublist]\n","#assign unique integer\n","vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>\n","\n","print(f\"before vocabs_len: {len(vocabs)}\")\n","vocabs.append('<UNK>')\n","print(f\"after vocabs_len: {len(vocabs)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNQAMib6wGzD","executionInfo":{"status":"ok","timestamp":1737274544352,"user_tz":-420,"elapsed":409,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"2ce1f0d5-4426-48e0-d08a-d342f7e18a42"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["14394"]},"metadata":{},"execution_count":94}],"source":["#create handy mapping between integer and word\n","word2index = {v:idx for idx, v in enumerate(vocabs)}\n","word2index['<UNK>']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0P-QTLLSwGzE","executionInfo":{"status":"ok","timestamp":1737274546940,"user_tz":-420,"elapsed":470,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"b71ef0b4-79fd-4e57-d80e-fb23e14eefd4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<UNK>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":95}],"source":["\n","index2word = {v:k for k, v in word2index.items()}\n","index2word[14394]"]},{"cell_type":"markdown","metadata":{"id":"pQnE9c85wGzE"},"source":["## 2. Build Co-occurence Matrix X"]},{"cell_type":"markdown","metadata":{"id":"pP-ex9KhwGzE"},"source":["Here, we need to count the co-occurence of two words given some window size.  We gonna use window size of 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrYmFdNPwGzE"},"outputs":[],"source":["from collections import Counter\n","\n","X_i = Counter(flatten(corpus))\n","#X_i"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wV_vJ-OlwGzE"},"outputs":[],"source":["# window_size = 2\n","# skip_grams = []\n","\n","# for doc in corpus:\n","#     for i in range(window_size, len(doc)-window_size):\n","#         center = doc[i]\n","#         outside = [word2index[doc[i+j]] for j in range(-window_size, window_size+1) if i+j != i]\n","#         for each_out in outside:\n","#             skip_grams.append((center, each_out))\n","# #skip_grams"]},{"cell_type":"code","source":["window_size = 2\n","\n","skip_grams = []\n","\n","#loop each corpus\n","for doc in corpus:\n","    #look from the 2nd word until second last word\n","    for i in range(window_size, len(doc)-window_size):\n","        #center word\n","        center = doc[i]\n","        #outside words = 2 words\n","\n","        outside = []\n","        for j in range(window_size):\n","            outside.append(doc[i+(j+1)])\n","            outside.append(doc[i-(j+1)])\n","\n","        #for each of these two outside words, we gonna append to a list\n","        for each_out in outside:\n","            skip_grams.append((center, each_out))\n","            #center, outside1;   center, outside2\n","\n","#skip_grams"],"metadata":{"id":"t-gMJLgCxXRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#skip_grams"],"metadata":{"id":"pZ-3O5rSwkDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZzlsfKIwGzE"},"outputs":[],"source":["X_ik_skipgrams = Counter(skip_grams)\n","#X_ik_skipgrams"]},{"cell_type":"markdown","metadata":{"id":"82hFDH6ZwGzE"},"source":["### Weighting function\n","\n","GloVe includes a weighting function to scale down too frequent words.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"im3DvdvSwGzE"},"outputs":[],"source":["def weighting(w_i, w_j, X_ik):\n","\n","    #check whether the co-occurences between w_i and w_j is available\n","    try:\n","        x_ij = X_ik[(w_i, w_j)]\n","        #if not exist, then set to 1 \"laplace smoothing\"\n","    except:\n","        x_ij = 1\n","\n","    #set xmax\n","    x_max = 100\n","    #set alpha\n","    alpha = 0.75\n","\n","    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n","    if x_ij < x_max:\n","        result = (x_ij / x_max)**alpha\n","    #otherwise, set to 1\n","    else:\n","        result = 1\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPJ04vtEwGzE"},"outputs":[],"source":["from itertools import combinations_with_replacement\n","\n","X_ik = {} #keeping the co-occurences\n","weighting_dic = {} #already scale the co-occurences using the weighting function\n","\n","for bigram in combinations_with_replacement(vocab, 2):\n","    if X_ik_skipgrams.get(bigram):  #if the pair exists in our corpus\n","        co = X_ik_skipgrams[bigram]\n","        X_ik[bigram] = co + 1 #for stability\n","        X_ik[(bigram[1], bigram[0])] = co + 1 #basically apple, banana = banana, apple\n","    else:\n","        pass\n","\n","    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n","    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"]},{"cell_type":"markdown","metadata":{"id":"eEbPamAVwGzE"},"source":["## 3. Prepare train data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbY2MHcZwGzE"},"outputs":[],"source":["import math\n","\n","def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n","\n","    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n","\n","    #convert our skipgrams to id\n","    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n","\n","    #randomly choose indexes based on batch size\n","    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n","\n","    #get the random input and labels\n","    for index in random_index:\n","        random_inputs.append([skip_grams_id[index][0]])\n","        random_labels.append([skip_grams_id[index][1]])\n","        #coocs\n","        pair = skip_grams[index] #e.g., ('banana', 'fruit')\n","        try:\n","            cooc = X_ik[pair]\n","        except:\n","            cooc = 1\n","        random_coocs.append([math.log(cooc)])\n","\n","        #weightings\n","        weighting = weighting_dic[pair]\n","        random_weightings.append([weighting])\n","\n","    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"]},{"cell_type":"markdown","metadata":{"id":"FWW8Y3KWwGzF"},"source":["### Testing the method"]},{"cell_type":"code","source":["#weighting_dic"],"metadata":{"id":"HaOr1x4gvkBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skip_grams[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKSDx8FLv0QQ","executionInfo":{"status":"ok","timestamp":1737275512836,"user_tz":-420,"elapsed":428,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"a368deb0-46e5-4379-8f33-1798d169cbc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('County', 'Grand')"]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["#skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]"],"metadata":{"id":"967f7YHWwPqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLsbYfvRwGzF"},"outputs":[],"source":["batch_size = 2\n","x, y, cooc, weighting = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lxg9-09owGzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737275515519,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"b43f52ab-9493-4f38-d229-54bb7c2126e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[9314],\n","       [2135]])"]},"metadata":{},"execution_count":146}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hSJTeFzwGzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737275515519,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"a7808312-ece6-41dd-ed38-5007440cbb85"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 729],\n","       [3004]])"]},"metadata":{},"execution_count":147}],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhiU_tFYwGzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737275516678,"user_tz":-420,"elapsed":5,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"a961765b-60f7-4d59-99ba-494b06a5c188"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.09861229],\n","       [1.09861229]])"]},"metadata":{},"execution_count":148}],"source":["cooc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7wwbV0kwGzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737275517675,"user_tz":-420,"elapsed":2,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"8b6afa05-7787-4dfc-ac62-3ae82c7dd96d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.07208434],\n","       [0.07208434]])"]},"metadata":{},"execution_count":149}],"source":["weighting"]},{"cell_type":"markdown","metadata":{"id":"TGSk1RwrwGzF"},"source":["## 4. Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9o5IQM0JwGzF"},"outputs":[],"source":["class Glove(nn.Module):\n","\n","    def __init__(self, voc_size, emb_size, word2index):\n","        super(Glove, self).__init__()\n","        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n","        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n","\n","        self.center_bias       = nn.Embedding(voc_size, 1)\n","        self.outside_bias      = nn.Embedding(voc_size, 1)\n","\n","        self.word2index = word2index\n","\n","    def forward(self, center, outside, coocs, weighting):\n","        center_embeds  = self.center_embedding(center) #(batch_size, 1, emb_size)\n","        outside_embeds = self.outside_embedding(outside) #(batch_size, 1, emb_size)\n","\n","        center_bias    = self.center_bias(center).squeeze(1)\n","        target_bias    = self.outside_bias(outside).squeeze(1)\n","\n","        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n","        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n","\n","        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n","\n","        return torch.sum(loss)\n","\n","    def get_embed(self, word):\n","        try:\n","            index = word2index[word]\n","        except:\n","            index = word2index['<UNK>']\n","\n","        word = torch.LongTensor([index])\n","\n","        embed_c = self.center_embedding(word)\n","        embed_o = self.outside_embedding(word)\n","        embed   = (embed_c + embed_o) / 2\n","\n","        return embed[0][0].item(), embed[0][1].item()"]},{"cell_type":"code","source":["len(vocabs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3AoHCe9peFO","executionInfo":{"status":"ok","timestamp":1737275922058,"user_tz":-420,"elapsed":574,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"c68f9844-09ac-4af4-f36d-297a602d709b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14395"]},"metadata":{},"execution_count":178}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQOhdhZrwGzF"},"outputs":[],"source":["#test our system\n","\n","voc_size = len(vocabs)\n","emb_size = 2\n","model = Glove(voc_size, emb_size,word2index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcTY7NQVwGzF"},"outputs":[],"source":["x_tensor = torch.LongTensor(x)\n","y_tensor = torch.LongTensor(y)\n","cooc_tensor = torch.FloatTensor(cooc)\n","weighting_tensor = torch.FloatTensor(weighting)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRzviSfXwGzF"},"outputs":[],"source":["loss = model(x_tensor, y_tensor, cooc_tensor, weighting_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ndw0hlcgwGzF","outputId":"37176cf1-f079-4221-eee4-6772f3baa1dd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737275940240,"user_tz":-420,"elapsed":4,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.1946, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":187}],"source":["loss"]},{"cell_type":"markdown","metadata":{"id":"b-eP3oFNwGzF"},"source":["## 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nl6jmBJtwGzF"},"outputs":[],"source":["batch_size     = 2 # mini-batch size\n","embedding_size = 2 #so we can later plot\n","model          = Glove(voc_size, embedding_size, word2index)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGCoKKWQwGzF"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_Nl6eJfwGzF","outputId":"baa518be-cea0-47a3-dd13-4fe3928bb7d5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737275996467,"user_tz":-420,"elapsed":43753,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 100 | Loss: 10.266328 | time: 0m 0s\n","Epoch: 200 | Loss: 72.676102 | time: 0m 0s\n","Epoch: 300 | Loss: 6.227177 | time: 0m 0s\n","Epoch: 400 | Loss: 1.782219 | time: 0m 0s\n","Epoch: 500 | Loss: 0.276304 | time: 0m 0s\n","Total time:  0 minutes 43 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","num_epochs = 500\n","for epoch in range(num_epochs):\n","\n","    start = time.time()\n","\n","    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n","    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n","    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n","    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n","    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n","\n","    optimizer.zero_grad()\n","    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    end = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start, end)\n","\n","    if (epoch + 1) % 100 == 0:\n","\n","      end = time.time()\n","      epoch_mins, epoch_secs = epoch_time(start, end)\n","\n","      print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n","      start = end\n","\n","end_time = time.time()\n","minutes, seconds = epoch_time(start_time, end_time)\n","print(f\"Total time: {minutes:2.0f} minutes {seconds:2.0f} seconds\")\n"]},{"cell_type":"markdown","metadata":{"id":"Sp4mavlGwGzF"},"source":["## 6. Testing"]},{"cell_type":"code","source":["vect = []\n","\n","for word in vocabs:\n","    vect.append(model.get_embed(word))\n","vect = np.array(vect)"],"metadata":{"id":"JWT0PaxmOaly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # ipython-input-54-07b52817ebdf\n","# vect = []\n","\n","# for word in vocabs:\n","#     try:\n","#         vect.append(model.get_embed(word)) # Get embedding for the current word\n","#     except IndexError:\n","#         # Handle IndexError, likely due to the word index being out of range\n","#         # This could happen if the word is not in the model's vocabulary\n","#         print(f\"Word '{word}' caused an IndexError. Skipping...\")\n","#         # Alternatively, you could assign a default embedding for unknown words\n","#         # For example: vect.append(model.get_embed('<UNK>'))\n","# vect = np.array(vect) # Convert the list of embeddings to a numpy array"],"metadata":{"id":"wFfW8uVJWzPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#scipy version\n","from scipy import spatial\n","\n","def cos_sim(a, b):\n","    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n","    return cos_sim\n","\n","def cos_sim_scores(vect_space, target_vect):\n","    scores = []\n","    for each_vect in vect_space:\n","        each_vect = tuple(each_vect)\n","        target_vect=tuple(target_vect)\n","        scores.append(cos_sim(target_vect, each_vect))\n","\n","    return np.array(scores)"],"metadata":{"id":"BIL3pv6QOlHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def similarity(model, data):\n","    words = data.split(\" \")\n","\n","    embed0 = np.array(model.get_embed(words[0]))\n","    embed1 = np.array(model.get_embed(words[1]))\n","    embed2 = np.array(model.get_embed(words[2]))\n","\n","    sim_vect = embed1 - embed0 + embed2\n","\n","    sim_scores = cos_sim_scores(vect, sim_vect)\n","    max_score_idx = np.argmax(sim_scores)\n","    sim_word = index2word[max_score_idx]\n","\n","    result = False\n","    if sim_word == words[3]:\n","        result = True\n","\n","    return result"],"metadata":{"id":"FbCCjMPgOpFW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Semantic Test"],"metadata":{"id":"AprF82fjOrVa"}},{"cell_type":"code","source":["semantic_file = \"data/word-test-semantic.txt\"\n","# open file\n","with open(semantic_file, \"r\") as file:\n","    sem_file = file.readlines()\n","    #send semantic into vector\n","\n","semantic = []\n","for sent in sem_file:\n","    semantic.append(sent.strip())\n","\n","#semantic"],"metadata":{"id":"lX33QMJsOr7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sem_count = len(semantic)\n","#sem_total\n","sem_correct = 0\n","for sent in semantic:\n","    if similarity(model, sent):\n","        sem_correct += 1"],"metadata":{"id":"rwySFNmoOt7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sem_accuracy = sem_correct / sem_count\n","print(f\"Semantic accuracy: {sem_accuracy:2.2f}\")\n","print(f\"Semantic correct: {sem_correct}\")\n","print(f\"Semantic count: {sem_count}\")"],"metadata":{"id":"v1hkyzbsOwY0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737276052380,"user_tz":-420,"elapsed":43,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"e0c30de8-5250-45fe-cb79-471acf769979"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic accuracy: 0.00\n","Semantic correct: 0\n","Semantic count: 506\n"]}]},{"cell_type":"markdown","source":["### Syntatic Test"],"metadata":{"id":"OLa6muRqOzvr"}},{"cell_type":"code","source":["syntatic_file = \"data/word-test-syntatic.txt\"\n","# open file\n","with open(syntatic_file, \"r\") as file:\n","    syn_file = file.readlines()\n","\n","syntatic = []\n","for sent in syn_file:\n","    syntatic.append(sent.strip())\n","#syntatic"],"metadata":{"id":"T2_jSkKPOx7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["syn_count = len(syntatic)\n","syn_correct = 0\n","for sent in syntatic:\n","    if similarity(model, sent):\n","        syn_correct += 1"],"metadata":{"id":"xsUoKgCwO0df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["syn_accuracy = syn_correct / syn_count\n","print(f\"Syntatic accuracy: {syn_accuracy:2.2f}\")\n","print(f\"Syntatic correct: {syn_correct}\")\n","print(f\"Syntatic count: {syn_count}\")"],"metadata":{"id":"l8fiQMrwO2L7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737276222159,"user_tz":-420,"elapsed":32,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"19f4111e-9b21-4a19-b531-cd8cb398c900"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Syntatic accuracy: 0.00\n","Syntatic correct: 0\n","Syntatic count: 1560\n"]}]},{"cell_type":"markdown","source":["### Similarity Test\n"],"metadata":{"id":"RhhqHCBzO4wx"}},{"cell_type":"code","source":["similarity_file = \"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n","# open file\n","with open(similarity_file, \"r\") as file:\n","    sim_file = file.readlines()\n","\n","similarity = []\n","for sent in sim_file:\n","    similarity.append(sent.strip())\n","#syntatic"],"metadata":{"id":"HGzep5rMO5fp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def similarity_test(model, test_data):\n","    words = test_data.split(\"\\t\")\n","\n","    embed0 = np.array(model.get_embed(words[0].strip()))\n","    embed1 = np.array(model.get_embed(words[1].strip()))\n","\n","    model_result = embed1 @ embed0.T\n","    sim_result = float(words[2].strip())\n","\n","    return sim_result, model_result"],"metadata":{"id":"nya3DFZsO7Kb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sim_scores = []\n","model_scores = []\n","for sent in similarity:\n","    sim_result, model_result = similarity_test(model, sent)\n","\n","    sim_scores.append(sim_result)\n","    model_scores.append(model_result)"],"metadata":{"id":"xj5kfpCxO8tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import spearmanr\n","\n","corr = spearmanr(sim_scores, model_scores)[0]\n","\n","print(f\"The correlation result is {corr:2.2f}.\")"],"metadata":{"id":"BqsGDasFO_3u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737276222161,"user_tz":-420,"elapsed":28,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"b8def96f-d719-442b-e762-75156c23a9d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The correlation result is 0.02.\n"]}]},{"cell_type":"markdown","source":["\n","## 7. Saving the model"],"metadata":{"id":"hQ2RKeBIPkFS"}},{"cell_type":"code","source":["# Saving the model for testing\n","torch.save(model.state_dict(), 'app/models/glove.model')"],"metadata":{"id":"3DjncQ1YPtmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove_args = {\n","    'voc_size': voc_size,\n","    'emb_size': emb_size,\n","    'word2index': word2index,\n","}\n"],"metadata":{"id":"-kskGsn8P8f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","pickle.dump(glove_args, open('app/models/glove.args', 'wb'))"],"metadata":{"id":"FSS6s31GP97y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove_args = pickle.load(open('app/models/glove.args', 'rb'))\n","model_glove = Glove(**glove_args)\n","model_glove.load_state_dict(torch.load('app/models/glove.model'))\n","\n","# Test the model\n","model_glove.get_embed('The')"],"metadata":{"id":"jSx1RgFhQC9r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737276394638,"user_tz":-420,"elapsed":956,"user":{"displayName":"Nyein Chan Aung","userId":"10271779707834005263"}},"outputId":"1027eb25-a85e-42eb-b6cb-5c74d36f5f1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-210-8fdb8e7de74c>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_glove.load_state_dict(torch.load('app/models/glove.model'))\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.046009063720703125, -0.2606179118156433)"]},"metadata":{},"execution_count":210}]},{"cell_type":"code","source":[],"metadata":{"id":"k45QcZkjQMMy"},"execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":0}